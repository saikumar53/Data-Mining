{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the categorical data\n",
    "def check_categorical(data):\n",
    "    categorical_index = []\n",
    "    for i,value in enumerate(data.dtypes):\n",
    "        if(value == 'object'):\n",
    "            categorical_index.append(i)\n",
    "    j = 0\n",
    "    data = data.to_numpy()\n",
    "\n",
    "    \n",
    "    while j < len(categorical_index):\n",
    "        temp = data[:,categorical_index[j]]\n",
    "        u_categorical = np.unique(temp)\n",
    "        \n",
    "        num_indexes = []\n",
    "        for i,value in enumerate(u_categorical):\n",
    "            num_indexes.append(i)\n",
    "        dictionary = dict(zip(u_categorical, num_indexes))\n",
    "        for k in range(len(temp)):\n",
    "            data[k][categorical_index[j]] = dictionary.get(data[k][categorical_index[j]]) + 1\n",
    "        j+=1\n",
    "    \n",
    "    return data.astype(float),categorical_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Mean and Standard Deviation\n",
    "def mean_std_helper(data,value):\n",
    "    mean = np.mean([row[0:-1] for row in data if row[-1] == value],axis = 0)\n",
    "    std  = np.std([row[0:-1] for row in data if row[-1] == value], axis = 0)\n",
    "    return mean,std\n",
    "\n",
    "def mean_std(train,categorical_index):\n",
    "    train_data= np.delete(train,categorical_index,axis=1)\n",
    "    test_data_float = np.delete(train,categorical_index,axis=1)\n",
    "    mean_0,std_0 = mean_std_helper(train_data,0)\n",
    "    mean_1,std_1 = mean_std_helper(train_data,1)\n",
    "    return {'mean_0': mean_0 , 'std_0': std_0 ,'mean_1': mean_1 , 'std_1': std_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the prior probabilities \n",
    "def calc_prior_prob(train,categorical_index,class_0,class_1):\n",
    "    prior_prob_class_1 = {}\n",
    "    prior_prob_class_0= {} \n",
    "    \n",
    "    if len(categorical_index) != 0:\n",
    "        prior_prob_class_1 = {}\n",
    "        prior_prob_class_0= {}    \n",
    "        for j in categorical_index:\n",
    "            prior_prob_class_1[j] = {}\n",
    "            prior_prob_class_0[j] = {}\n",
    "            for k in np.unique(train[:,j]):\n",
    "                prior_0 = float(list(class_0[:,j]).count(k))/len(class_0)\n",
    "                prior_1 = float(list(class_1[:,j]).count(k))/len(class_1)\n",
    "                prior_prob_class_0[j][k] = prior_0\n",
    "                prior_prob_class_1[j][k] = prior_1\n",
    "                \n",
    "    prior_prob_0 = float(list(train[:,-1]).count(0))/len(train)\n",
    "    prior_prob_1 = float(list(train[:,-1]).count(1))/len(train)\n",
    "    \n",
    "    return {'prior_0' : prior_prob_0,'prior_1':prior_prob_1,'prior_class0':prior_prob_class_0,'prior_class1':prior_prob_class_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the probailities\n",
    "def calc_probabilty(mean, std_dev, test):    \n",
    "    prob = test[:,0:-1] - mean\n",
    "    prob = np.multiply(prob,prob)\n",
    "    prob = -1 * prob / (2 * np.multiply(std_dev,std_dev))\n",
    "    prob = np.exp(prob)\n",
    "    prob = prob/(math.sqrt(math.pi*2)*std_dev)\n",
    "    prob = np.prod(prob, axis = 1)\n",
    "    return prob\n",
    "\n",
    "def get_probability(test,prior_prob,meanStd,categorical_index):\n",
    "    test_data = np.delete(test,categorical_index,axis=1)\n",
    "    \n",
    "    cat_data_prob0 = np.empty(test.shape[0])\n",
    "    cat_data_prob0.fill(1.0)\n",
    "    cat_data_prob1 = np.empty(test.shape[0])\n",
    "    cat_data_prob1.fill(1.0)\n",
    "    \n",
    "#     print(cat_data_prob0)\n",
    "#     print(cat_data_prob1)\n",
    "    \n",
    "    if len(categorical_index) != 0:\n",
    "        for t in range(len(test)):\n",
    "            for i in categorical_index:\n",
    "                cat_data_prob1[t] *= prior_prob['prior_class1'][i][test[t][i]]\n",
    "                cat_data_prob0[t] *= prior_prob['prior_class0'][i][test[t][i]]\n",
    "    \n",
    "    t0 = np.multiply(calc_probabilty(meanStd['mean_0'], meanStd['std_0'], test_data), cat_data_prob0)\n",
    "    t1 = np.multiply(calc_probabilty(meanStd['mean_1'], meanStd['std_1'], test_data) , cat_data_prob1)\n",
    "    \n",
    "    prob_0 = prior_prob['prior_0'] * t0\n",
    "    prob_1 = prior_prob['prior_1'] * t1\n",
    "    \n",
    "    return {'prob0':prob_0,'prob1':prob_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation of Confusion Matrix\n",
    "def confusion_matrix(original, predicted):\n",
    "    accuracy = precision = recall = f_measure = 0\n",
    "    TP = FN = FP = TN = 0\n",
    "    for i in range(len(original)):\n",
    "        if original[i] == 1 and predicted[i] == 1:\n",
    "            TP += 1\n",
    "        elif original[i] == 1 and predicted[i] == 0:\n",
    "            FN += 1\n",
    "        elif original[i] == 0 and predicted[i] == 1:\n",
    "            FP += 1\n",
    "        else:\n",
    "            TN += 1\n",
    "            \n",
    "    if(TP + FN + FP + TN) != 0:\n",
    "        accuracy = (float(TP + TN)/(TP + FN + FP + TN))\n",
    "    if(TP + FP) != 0:\n",
    "        precision = (float(TP)/(TP + FP))\n",
    "    if(TP + FN)!= 0:\n",
    "        recall = (float(TP)/(TP + FN))\n",
    "    f_measure = (float(2 * TP) / ((2 * TP) + FN + FP))\n",
    "            \n",
    "    return accuracy, precision, recall, f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results for the file project3_dataset1.txt --------------\n",
      "Accuracy : 93.48997493734333\n",
      "Precision: 91.78709362532891\n",
      "Recall : 90.44426356826324\n",
      "F-1 Measure: 91.00318381263209\n"
     ]
    }
   ],
   "source": [
    "#Main Function\n",
    "def __main__():\n",
    "    accuracy = precision = recall = f_measure = 0\n",
    "    predicted_labels = []\n",
    "    \n",
    "    #Load the file in a Data Frame\n",
    "    file = 'project3_dataset1.txt'\n",
    "    data = pd.read_csv(file,sep=\"\\t\", header=None)    \n",
    "    data = data.iloc[:,:data.shape[1]]\n",
    "    data, categorical_index = check_categorical(data)\n",
    "\n",
    "    \n",
    "    #Number of Folds\n",
    "    folds = np.array_split(data,10)\n",
    "    \n",
    "    #Number of loops\n",
    "    i=0\n",
    "    while(i<len(folds)):\n",
    "        class_0 = []\n",
    "        class_1 = []\n",
    "        \n",
    "        #test_set\n",
    "        test = np.asarray(folds[i])\n",
    "        train = []\n",
    "        predicted_labels = []\n",
    "        for index,value in enumerate(folds):\n",
    "            if(index != i):\n",
    "                train.append(value)\n",
    "                \n",
    "        #train_set\n",
    "        train = np.asarray(np.vstack(train))\n",
    "        meanStd = mean_std(train,categorical_index)\n",
    "    \n",
    "        for i1 in range(len(train)):\n",
    "            if(int(train[i1,-1])) == 1:\n",
    "                class_1.append(train[i1,:])\n",
    "            elif (int(train[i1,-1])) == 0:\n",
    "                class_0.append(train[i1,:])\n",
    "                \n",
    "        class_0 = np.asarray(class_0)\n",
    "        class_1 = np.asarray(class_1)\n",
    "        \n",
    "        \n",
    "        #Calculation of probabilities\n",
    "        prior_probilities = calc_prior_prob(train,categorical_index,class_0,class_1)\n",
    "        probabilities = get_probability(test,prior_probilities,meanStd,categorical_index)\n",
    "\n",
    "        prob0 = probabilities['prob0']\n",
    "        prob1 = probabilities['prob1']\n",
    "     \n",
    "        #Estimated Predicted Labels\n",
    "        for j in range(len(test)):\n",
    "            _ = predicted_labels.append(1) if prob1[j] > prob0[j] else predicted_labels.append(0)\n",
    "\n",
    "        #Accuracy, Precision, Recall, Precision calculations\n",
    "        acc, prec, rec, f_meas = confusion_matrix(test[:,-1], np.asarray(predicted_labels))\n",
    "        accuracy += acc\n",
    "        precision += prec\n",
    "        recall += rec\n",
    "        f_measure += f_meas\n",
    "                \n",
    "        i+=1\n",
    "        \n",
    "    #Outputs\n",
    "    print('Naive Bayes Results for the file {} --------------'.format(file))\n",
    "    print('Accuracy : {}'.format(str(accuracy * 10)))\n",
    "    print('Precision: {}'.format(str(precision * 10)))\n",
    "    print('Recall : {}'.format(str(recall * 10)))\n",
    "    print('F-1 Measure: {}'.format(str(f_measure * 10)))\n",
    "\n",
    "__main__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
